--- Page 494 ---
第14章 云原生架构设计理论与实践
业看重的办公楼、厂房、IT设施等有形资产，其重要性也逐渐被这些云端数字资产所超越，企
业正通过云原生构建一个完整的数字孪生的新体系，而这才是云原生技术的真正价值所在。
所有这些问题都指向一个共同点，那就是云的时代需要新的技术架构，来帮助企业应用能
够更好地利用云计算优势，充分释放云计算的技术红利，让业务更敏捷、成本更低的同时又可
伸缩性更灵活，而这些正好就是云原生架构专注解决的技术点。
对于整个云计算产业的发展本身来说，云原生区别于早先的虚拟机阶段，也完成了一次全
新的技术生产力变革，是从云技术的应用特性和交付架构上进行了创新性的组合，能够极大地
释放云计算的生产能力。此外，云原生的变革从一开始自然而然地与开源生态走在了一起，也
意味着云原生技术从一开始就选择了一条“飞轮进化”式的道路，通过技术的易用性和开放性
实现快速增长的正向循环，又通过不断壮大的应用实例来推动了企业业务全面上云和自身技术
版图的不断完善。云原生所带来的种种好处，对于企业的未来业务发展的优势，已经成为众多
企业的新共识。可以预见，更多企业在经历了这一轮云原生的变革之痛后，能够穿越企业的原
有成长周期，跨越到数字经济的新赛道，在即将到来的全面云化的数字时代更好地开发业务。
开源项目的不断更新和逐步成熟，也促使各企业在AI、大数据、边缘、高性能计算等新兴
业务场景不断采用云原生技术来构建创新解决方案。
大量企业尝试使用容器替换现有人工智能、大数据的基础平台，通过容器更小粒度的资源
划分、更快的扩容速度、更灵活的任务调度，以及天然的计算与存储分离架构等特点，助力人
工智能、大数据在业务性能大幅提升的同时，更好地控制成本。各云厂商也相继推出了对应的
容器化服务，比如华为云的AI容器、大数据容器，AWS的深度学习容器等。
云原生技术与边缘计算相结合，可以比较好地解决传统方案中轻量化、异构设备管理、海
量应用运维管理的难题，如目前国内最大的边缘计算落地项目—国家路网中心的全国高速公
路取消省界收费站项目，就使用了基于云原生技术的边缘计算解决方案，解决了10万+异构设
备管理、30多万边缘应用管理的难题。主流的云计算厂商也相继推出了云原生边缘计算解决方
案，如华为云智能边缘平台IEF、AWS的GreenGrass、阿里云的ACK@Edge等等。
云原生在高性能计算(HPC)领域的应用呈现出快速上升的势头。云原生在科研及学术机
构、生物、制药等行业率先得到应用，例如欧洲核子研究中心(CERN)、中国科学院上海生命
科学研究院、中国农业大学、华大基因、未来组等单位都已经将传统的高性能计算业务升级为
云原生架构。为了更好地支撑高性能计算场景，各云计算厂商也纷纷推出面向高性能计算专场
的云原生解决方案。
云原生与商业场景的深度融合，不仅为各行业注入了发展与创新的新动能，也促使云原生
技术更快发展、生态更加成熟，主要表现为以下几点：
(1)从为企业带来的价值来看，云原生架构有着以下优势通过对多元算力的支持，满足不
同应用场景的个性化算力需求，并基于软硬协同架构，为应用提供极致性能的云原生算力；基
于多云治理和边云协同，打造高效、高可靠的分布式泛在计算平台，并构建包括容器、裸机、
虚机、函数等多种形态的统一计算资源：以“应用”为中心打造高效的资源调度和管理平台，
为企业提供一键式部署、可感知应用的智能化调度，以及全方位监控与运维能力。
(2)通过最新的 DevSecOps应用开发模式，实现了应用的敏捷开发，提升业务应用的迭代
483
--- Page 495 ---
484 系统架构设计师教程(第2版)
速度，高效响应用户需求，并保证全流程安全。对于服务的集成提供侵入和非侵入两种模式辅
助企业应用架构升级，同时实现新老应用的有机协同，立而不破。
(3)帮助企业管理好数据，快速构建数据运营能力，实现数据的资产化沉淀和价值挖掘，
并借助一系列AI技术，再次赋能给企业应用，结合数据和AI的能力帮助企业实现业务的智能
升级。
(4)结合云平台全方位企业级安全服务和安全合规能力，保障企业应用在云上安全构建，
业务安全运行。
14.2 云原生架构内涵
关于云原生的定义有众多版本，云原生架构的理解也不尽相同，本书将根据广泛的云原生
技术、产品和上云实践，给出一般性的理解。
14.2.1 云原生架构定义
从技术的角度，云原生架构是基于云原生技术的一组架构原则和设计模式的集合，旨在将
云应用中的非业务代码部分进行最大化的剥离，从而让云设施接管应用中原有的大量非功能特
性(如弹性、韧性、安全、可观测性、灰度等),使业务不再有非功能性业务中断困扰的同时，
具备轻量、敏捷、高度自动化的特点。由于云原生是面向“云”而设计的应用，因此，技术部
分依赖于传统云计算的3层概念，即基础设施即服务(IaaS)、平台即服务(PaaS)和软件即服
务(SaaS)。
云原生的代码通常包括三部分：业务代码、三方软件、处理非功能特性的代码。其中“业
务代码”指实现业务逻辑的代码；“三方软件”是业务代码中依赖的所有三方库，包括业务库和
基础库；“处理非功能性的代码”指实现高可用、安全、可观测性等非功能性能力的代码。三部
分中只有业务代码是核心，是对业务真正带来价值的，另外两个部分都只算附属物，但是，随
着软件规模的增大、业务模块规模变大、部署环境增多、分布式复杂性增强，使得今天的软件
构建变得越来越复杂，对开发人员的技能要求也越来越高。云原生架构相比较传统架构进了一
大步，从业务代码中剥离大量非功能性特性(不会是所有，比如易用性还不能剥离)到IaaS和
PaaS中，从而减少业务代码开发人员的技术关注范围，通过云厂商的专业性提升应用的非功能
性能力。
此外，具备云原生架构的应用可以最大程度利用云服务和提升软件交付能力，进一步加快
软件开发。
1.代码结构发生巨大变化
云原生架构产生的最大影响就是让开发人员的编程模型发生了巨大变化。今天大部分的编
程语言中，都有文件、网络、线程等元素，这些元素为充分利用单机资源带来好处的同时，也
提升了分布式编程的复杂性；因此大量框架、产品涌现，来解决分布式环境中的网络调用问题、
高可用问题、CPU争用问题、分布式存储问题，等等。
--- Page 496 ---
第14章 云原生架构设计理论与实践 485
在云的环境中，“如何获取存储”变成了若干服务，包括对象存储服务、块存储服务和没有
随机访问的文件存储服务。云不仅改变了开发人员获得这些存储能力的界面，还在于云产品解
决了分布式场景中的各种挑战，包括高可用挑战、自动扩缩容挑战、安全挑战、运维升级挑战
等，应用的开发人员不用在其代码中处理节点宕机前如何把本地保存的内容同步到远端的问题，
也不用处理当业务峰值到来时如何对存储节点进行扩容的问题，而应用的运维人员不用在发现
zeroday 安全问题时紧急对三方存储软件进行升级。
云把三方软硬件的能力升级成了服务，开发人员的开发复杂度和运维人员的运维工作量都
得到极大降低。显然，如果这样的云服务用得越多，那么开发和运维人员的负担就越少，企业
在非核心业务实现上从必须的负担变成了可控支出。在一些开发能力强的公司中，对这些三方
软硬件能力的处理往往是交给应用框架(或者说公司内自己的中间件)来做的；在云的时代云
厂商提供了更具SLA的服务，使得所有软件公司都可以由此获益。
这些使得业务代码的开发人员技能栈中，不再需要掌握文件及其分布式处理技术，不再需
要掌握各种复杂的网络技术，简化让业务开发变得更敏捷、更快速。
2.非功能性特性大量委托
任何应用都提供两类特性，功能性特性和非功能性特性。功能性特性是真正为业务带来价
值的代码，比如建立客户资料、处理订单、支付等；即使是一些通用的业务功能特性，比如组
织管理、业务字典管理、搜索等也是紧贴业务需求的。非功能性特性是没有给业务带来直接业
务价值，但通常又是必不可少的特性，比如高可用能力、容灾能力、安全特性、可运维性、易
用性、可测试性、灰度发布能力等。
云计算虽然没有解决所有非功能性问题，但确实有大量非功能性，特别是分布式环境下复
杂非功能性问题，被云产品解决了。以大家最头疼的高可用为例，云产品在多个层面为应用提
供了解决方案。
虚拟机：当虚拟机检测到底层硬件发生异常时，自动帮助应用做热迁移，迁移后的应用不
需重新启动而仍然具备对外服务的能力，应用对整个迁移过程都不会有任何感知。
容器：有时应用所在的物理机是正常的，只是应用自身的问题(比如 bug、资源耗尽等)
而无法正常对外提供服务。容器通过监控检查探测到进程状态异常，从而实施异常节点的下线、
新节点上线和生产流量的切换等操作，整个过程自动完成而无需运维人员干预。
云服务：如果应用把“有状态”部分都交给了云服务(如缓存、数据库、对象存储等),加
上全局对象的持有小型化或具备从磁盘快速重建能力，由于云服务本身是具备极强的高可用能
力，那么应用本身会变成更薄的“无状态”应用，高可用故障带来的业务中断会降至分钟级；
如果应用是N-M的对等架构模式，那么结合负载均衡产品可获得很强的高可用能力。
3.高度自动化的软件交付
软件一旦开发完成，需要在公司内外部各类环境中部署和交付，以将软件价值交给最终客
户。软件交付的困难在于开发环境到生产环境的差异(公司环境到客户环境之间的差异)以及
软件交付和运维人员的技能差异，填补这些差异的是一大堆安装手册、运维手册和培训文档。
容器以一种标准的方式对软件打包，容器及相关技术则帮助屏蔽不同环境之间的差异，进而基
--- Page 497 ---
486 系统架构设计师教程(第2版)
于容器做标准化的软件交付。
对自动化交付而言，还需要一种能够描述不同环境的工具，让软件能够“理解”目标环境、
交付内容、配置清单并通过代码去识别目标环境的差异，根据交付内容以“面向终态”的方式
完成软件的安装、配置、运行和变更。
基于云原生的自动化软件交付相比较当前的人工软件交付是一个巨大的进步。以微服务为
例，应用微服务化以后，往往被部署到成千上万个结点上，如果系统不具备高度的自动化能力，
任何一次新业务的上线，都会带来极大的工作量挑战，严重时还会导致业务变更超过上线窗口
而不可用。
14.2.2 云原生架构原则
云原生架构本身作为一种架构，也有若干架构原则作为应用架构的核心架构控制面，通过
遵从这些架构原则可以让技术主管和架构师在做技术选择时不会出现大的偏差。
1.服务化原则
当代码规模超出小团队的合作范围时，就有必要进行服务化拆分了，包括拆分为微服务架
构、小服务(MiniService)架构，通过服务化架构把不同生命周期的模块分离出来，分别进行
业务迭代，避免迭代频繁模块被慢速模块拖慢，从而加快整体的进度和稳定性。同时服务化架
构以面向接口编程，服务内部的功能高度内聚，模块间通过公共功能模块的提取增加软件的复
用程度。
分布式环境下的限流降级、熔断隔仓、灰度、反压、零信任安全等，本质上都是基于服务
流量(而非网络流量)的控制策略，所以云原生架构强调使用服务化的目的还在于从架构层面
抽象化业务模块之间的关系，标准化服务流量的传输，从而帮助业务模块进行基于服务流量的
策略控制和治理，不管这些服务是基于什么语言开发的。
2.弹性原则
大部分系统部署上线需要根据业务量的估算，准备一定规模的机器，从提出采购申请，到
供应商洽谈、机器部署上电、软件部署、性能压测，往往需要好几个月甚至一年的周期；而这
期间如果业务发生变化了，重新调整也非常困难。弹性则是指系统的部署规模可以随着业务量
的变化而自动伸缩，无须根据事先的容量规划准备固定的硬件和软件资源。好的弹性能力不仅
缩短了从采购到上线的时间，让企业不用操心额外软硬件资源的成本支出(闲置成本),降低了
企业的IT成本，更关键的是当业务规模面临海量突发性扩张的时候，不再因为平时软硬件资源
储备不足而“说不”,保障了企业收益。
3.可观测原则
大部分企业的软件规模都在不断增长，原来单机可以对应用做完所有调试，但在分布式环
境下需要对多个主机上的信息做关联，才可能回答清楚服务为什么宕机，哪些服务违反了其定
义的SLO(Service Level Objective,服务等级目标),目前的故障影响哪些用户，最近这次变更
对哪些服务指标带来了影响等问题，这些都要求系统具备更强的可观测能力。可观测性与监控、
--- Page 498 ---
第14章 云原生架构设计理论与实践
业务探活、APM等系统提供的能力不同，前者是在云这样的分布式系统中，主动通过日志、链
路跟踪和度量等手段，使得一次点击背后的多次服务调用的耗时、返回值和参数都清晰可见，
甚至可以下钻到每次三方软件调用、SQL请求、节点拓扑、网络响应等，这样的能力可以使运
维、开发和业务人员实时掌握软件运行情况，并结合多个维度的数据指标，获得前所未有的关
联分析能力，不断对业务健康度和用户体验进行数字化衡量和持续优化。
4.韧性原则
当业务上线后，最不能接受的就是业务不可用，让用户无法正常使用软件，影响体验和
收入。韧性代表了当软件所依赖的软硬件组件出现各种异常时，软件表现出来的抵御能力，
这些异常通常包括硬件故障、硬件资源瓶颈(如CPU/网卡带宽耗尽)、业务流量超出软件设
计能力、影响机房工作的故障和灾难、软件 bug、黑客攻击等对业务不可用带来致命影响的
因素。
韧性从多个维度诠释了软件持续提供业务服务的能力，核心目标是提升软件的平均无故障
时间(Mean Time Between Failure,MTBF)。从架构设计上，韧性包括服务异步化能力、重试/
限流/降级/熔断/反压、主从模式、集群模式、AZ内的高可用、单元化、跨 region容灾、异
地多活容灾等。
5.所有过程自动化原则
技术往往是把“双刃剑”,容器、微服务、DevOps、大量第三方组件的使用，在降低分布
式复杂性和提升迭代速度的同时，因为整体增大了软件技术栈的复杂度和组件规模，所以不
可避免地带来了软件交付的复杂性，如果这里控制不当，应用就无法体会到云原生技术的优
势。通过IaC(Infrastructure as Code)、GitOps、OAM(Open Application Model)、Kubernetes
Operator和大量自动化交付工具在CICD流水线中的实践，一方面标准化企业内部的软件交付
过程，另一方面在标准化的基础上进行自动化，通过配置数据自描述和面向终态的交付过程，
让自动化工具理解交付目标和环境差异，实现整个软件交付和运维的自动化。
6.零信任原则
零信任安全针对传统边界安全架构思想进行了重新评估和审视，并对安全架构思路给出了
新建议。其核心思想是，默认情况下不应该信任网络内部和外部的任何人/设备/系统，需要
基于认证和授权重构访问控制的信任基础，诸如IP地址、主机、地理位置、所处网络等均不能
作为可信的凭证。零信任对访问控制进行了范式上的颠覆，引导安全体系架构从“网络中心化”
走向“身份中心化”,其本质诉求是以身份为中心进行访问控制。
零信任第一个核心问题就是身份(Identity),赋予不同的实体不同的身份，解决是谁在什
么环境下访问某个具体的资源的问题。在研发、测试和运维微服务场景下，身份及其相关策略
不仅是安全的基础，更是众多(资源、服务、环境)隔离机制的基础；在员工访问企业内部应
用的场景下，身份及其相关策略提供了即时的接入服务。
7.架构持续演进原则
今天技术和业务的演进速度非常快，很少有一开始就清晰定义了架构并在整个软件生命周
487
--- Page 499 ---
488 系统架构设计师教程(第2版)
期里面都适用，相反往往还需要对架构进行一定范围内的重构，因此云原生架构本身也必须是
一个具备持续演进能力的架构，而不是一个封闭式架构。除了增量迭代、目标选取等因素外，
还需要考虑组织(例如架构控制委员会)层面的架构治理和风险控制，特别是在业务高速迭
代情况下的架构、业务、实现平衡关系。云原生架构对于新建应用而言的架构控制策略相对
容易选择(通常是选择弹性、敏捷、成本的维度),但对于存量应用向云原生架构迁移，则需
要从架构上考虑遗留应用的迁出成本/风险和到云上的迁入成本/风险，以及技术上通过微服
务/应用网关、应用集成、适配器、服务网格、数据迁移、在线灰度等应用和流量进行细颗粒
度控制。
14.2.3 主要架构模式
云原生架构有非常多的架构模式，这里选取一些对应用收益更大的主要架构模式进行讨论。
1.服务化架构模式
服务化架构是云时代构建云原生应用的标准架构模式，要求以应用模块为颗粒度划分一个
软件，以接口契约(例如IDL)定义彼此业务关系，以标准协议(HTTP、gRPC等)确保彼此
的互联互通，结合DDD(领域模型驱动)、TDD(测试驱动开发)、容器化部署提升每个接口的
代码质量和迭代速度。服务化架构的典型模式是微服务和小服务模式，其中小服务可以看作是
一组关系非常密切的服务的组合，这组服务会共享数据，小服务模式通常适用于非常大型的软
件系统，避免接口的颗粒度太细而导致过多的调用损耗(特别是服务间调用和数据一致性处理)
和治理复杂度。
通过服务化架构，把代码模块关系和部署关系进行分离，每个接口可以部署不同数量的实
例，单独扩缩容，从而使得整体的部署更经济。此外，由于在进程级实现了模块的分离，每个
接口都可以单独升级，从而提升了整体的迭代效率。但也需要注意，服务拆分导致要维护的模
块数量增多，如果缺乏服务的自动化能力和治理能力，会让模块管理和组织技能不匹配，反而
导致开发和运维效率的降低。
2.Mesh 化架构模式
Mesh化架构是把中间件框架(如RPC、缓存、异步消息等)从业务进程中分离，让中间件
SDK与业务代码进一步解耦，从而使得中间件升级对业务进程没有影响，甚至迁移到另外一个
平台的中间件也对业务透明。分离后在业务进程中只保留很“薄”的Client 部分，Client 通常很
少变化，只负责与Mesh进程通信，原来需要在SDK中处理的流量控制、安全等逻辑由Mesh
进程完成。整个架构如图14-1所示。
实施Mesh化架构后，大量分布式架构模式(熔断、限流、降级、重试、反压、隔
仓……)都由Mesh 进程完成，即使在业务代码的制品中并没有使用这些三方软件包；同时获
得更好的安全性(比如零信任架构能力)、按流量进行动态环境隔离、基于流量做冒烟/回归
测试等。
--- Page 500 ---
第14章 云原生架构设计理论与实践
业务进程
业务代码
RPC Redis
SDK SDK MQ DB
SDKSDK
SDK协议
网络
业务进程
业务代码
RPC RedisSDK SDK MQ DB
SDK协议、标准协议
Mcsh进程
(流量控制、安全策略、微隔离……)
SDK SDK
新协议、加密、染色……
传统架构 网络
Mesh化架构
图14-1 Mesh化架构
3.Serverless模式
Serverless将“部署”这个动作从运维中“收走”,使开发者不用关心应用运行地点、操作
系统、网络配置、CPU性能等，从架构抽象上看，当业务流量到来/业务事件发生时，云会启
动或调度一个已启动的业务进程进行处理，处理完成后云自动会关闭/调度业务进程，等待下
一次触发，也就是把应用的整个运行都委托给云。
Serverless并非适用任何类型的应用，因此架构决策者需要关心应用类型是否适合于
Serverless运算。如果应用是有状态的，由于Serverless的调度不会帮助应用做状态同步，因此
云在进行调度时可能导致上下文丢失；如果应用是长时间后台运行的密集型计算任务，会无法
发挥Serverless的优势；如果应用涉及频繁的外部I/O(网络或者存储，以及服务间调用),也
因为繁重的I/O负担、时延大而不适合。事件驱动架构图如图14-2所示。Serverless 非常适合于
事件驱动的数据计算任务、计算时间短的请求/响应应用、没有复杂相互调用的长周期任务。
生产者 主题
←—订阅—
—事件→ —事件- →
—事件→
—事件→
—事件→
—事件→
图14-2 事件驱动架构
4.存储计算分离模式
分布式环境中的CAP困难主要是针对有状态应用，因为无状态应用不存在C(一致性)这
个维度，因此可以获得很好的A(可用性)和P(分区容错性),因而获得更好的弹性。在云
环境中，推荐把各类暂态数据(如session)、结构化和非结构化持久数据都采用云服务来保存，
从而实现存储计算分离。但仍然有一些状态如果保存到远端缓存，会造成交易性能的明显下降，
消费者
489
--- Page 501 ---
490 系统架构设计师教程(第2版)
比如交易会话数据太大、需要不断根据上下文重新获取等，这时可以考虑通过采用时间日志+
快照(或检查点)的方式，实现重启后快速增量恢复服务，减少不可用对业务的影响时长。
5.分布式事务模式
微服务模式提倡每个服务使用私有的数据源，而不是像单体这样共享数据源，但往往大颗
粒度的业务需要访问多个微服务，必然带来分布式事务问题，否则数据就会出现不一致。架构
师需要根据不同的场景选择合适的分布式事务模式。
(1)传统采用XA模式，虽然具备很强的一致性，但是性能差。
(2)基于消息的最终一致性(BASE)通常有很高的性能，但是通用性有限。
(3)TCC模式完全由应用层来控制事务，事务隔离性可控，也可以做到比较高效；但是对
业务的侵入性非常强，设计开发维护等成本很高。
(4)SAGA模式与TCC模式的优缺点类似但没有try这个阶段，而是每个正向事务都对应
一个补偿事务，也是开发维护成本高。
(5)开源项目 SEATA的AT模式非常高性能且无代码开发工作量，且可以自动执行回滚操
作，同时也存在一些使用场景限制。
6.可观测架构
可观测架构包括Logging、Tracing、Metrics三个方面，其中Logging提供多个级别(verbose/
debug/warning/error/fatal)的详细信息跟踪，由应用开发者主动提供；Tracing提供一个请求从前
端到后端的完整调用链路跟踪，对于分布式场景尤其有用；Metrics 则提供对系统量化的多维度
度量。
架构决策者需要选择合适的、支持可观测的开源框架(比如Open Tracing、Open Telemetry
等),并规范上下文的可观测数据规范(例如方法名、用户信息、地理位置、请求参数等),规
划这些可观测数据在哪些服务和技术组件中传播，利用日志和 tracing信息中的spanid/traceid,
确保进行分布式链路分析时有足够的信息进行快速关联分析。
由于建立可观测性的主要目标是对服务SLO(Service Level Objective)进行度量，从而优
化 SLA,因此架构设计上需要为各个组件定义清晰的SLO,包括并发度、耗时、可用时长、容
量等。
7.事件驱动架构
事件驱动架构(EDA,Event Driven Architecture)本质上是一种应用/组件间的集成架构模式。
事件和传统的消息不同，事件具有schema,所以可以校验 event的有效性，同时 EDA具备
QoS保障机制，也能够对事件处理失败进行响应。事件驱动架构不仅用于(微)服务解耦，还
可应用于下面的场景中。
(1)增强服务韧性：由于服务间是异步集成的，也就是下游的任何处理失败甚至宕机都不
会被上游感知，自然也就不会对上游带来影响。
(2)CQRS(Command Query Responsibility Segregation):把对服务状态有影响的命令用事
件来发起，而对服务状态没有影响的查询才使用同步调用的API接口；结合EDA中的Event
--- Page 502 ---
第14章 云原生架构设计理论与实践
Sourcing机制可以用于维护数据变更的一致性，当需要重新构建服务状态时，把EDA中的事件
重新“播放”一遍即可。
(3)数据变化通知：在服务架构下，往往一个服务中的数据发生变化，另外的服务会感兴
趣，比如用户订单完成后，积分服务、信用服务等都需要得到事件通知并更新用户积分和信用
等级。
(4)构建开放式接口：在EDA下，事件的提供者并不用关心有哪些订阅者，不像服务调用
的场景——数据的产生者需要知道数据的消费者在哪里并调用它，因此保持了接口的开放性。
(5)事件流处理：应用于大量事件流(而非离散事件)的数据分析场景，典型应用是基于
Kafka的日志处理。
基于事件触发的响应：在IoT时代大量传感器产生的数据，不会像人机交互一样需要等待
处理结果的返回，天然适合用EDA来构建数据处理应用。
14.2.4 典型的云原生架构反模式
技术往往像一把双刃剑，企业做云原生架构演进的时候，会充分考虑根据不同的场景选择
不同的技术，下面是一些典型云原生架构反模式。
1.庞大的单体应用
庞大单体应用的最大问题在于缺乏依赖隔离，包括代码耦合带来的责任不清、模块间接口
缺乏治理而带来变更影响扩散、不同模块间的开发进度和发布时间要求难以协调、一个子模块
不稳定导致整个应用都变慢、扩容时只能整体扩容而不能对达到瓶颈的模块单独扩容等。因此
当业务模块可能存在多人开发的时候，就需要考虑通过服务化进行一定的拆分，梳理聚合根，
通过业务关系确定主要的服务模块以及这些模块的边界、清晰定义模块之间的接口，并让组织
关系和架构关系匹配。
2.单体应用“硬拆”为微服务
服务的拆分需要适度，过分服务化拆分反而会导致新架构与组织能力的不匹配，让架构升
级得不到技术红利，典型的例子包括：
(1)小规模软件的服务拆分：软件规模不大，团队人数也少，但是为了微服务化，强行把耦
合度高、代码量少的模块进行服务化拆分，一次性的发布需要拆分为多个模块分开发布和维护。
(2)数据依赖：服务虽然拆分为多个，但是这些服务的数据是紧密耦合的，于是让这些服
务共享数据库，导致数据的变化往往被扇出到多个服务中，造成服务间数据依赖。
(3)性能降低：当耦合性很强的模块被拆分为多个微服务后，原来的本地调用变成了分布
式调用，从而让响应时间变大了上千倍，导致整个服务链路性能急剧下降。
3.缺乏自动化能力的微服务
软件架构中非常重要的一个维度就是处理软件复杂度问题，一旦问题规模提升了很多，那
就必须重新考虑与之适应的新方案。在很多软件组织中，开发、测试和运维的工作单位都是以
进程为单位，比如把整个用户管理作为一个单独的模块进行打包、发布和运行；而进行了微服
491
--- Page 503 ---
492 系统架构设计师教程(第2版)
务拆分后，这个用户管理模块可能被分为用户信息管理、基本信息管理、积分管理、订单管理
等多个模块，由于仍然是每个模块分别打包、发布和运行，开发、测试和运维人员的人均负责
模块数就会直线上升，造成了人均工作量增大，也就增加了软件的开发成本。
实际上，当软件规模进一步变大后，自动化能力的缺失还会带来更大的危害。由于接口增
多会带来测试用例的增加，更多的软件模块排队等待测试和发布，如果缺乏自动化会造成软件
发布时间变长，在多环境发布或异地发布时更是需要专家来处理环境差异带来的影响。同时更
多的进程运行于一个环境中，缺乏自动化的人工运维容易给环境带来不可重现的影响，而一旦
发生人为运维错误又不容易“快速止血”,造成了故障处理时间变长，以及使得日常运维操作都
需要专家才能完成。所有这些问题都会导致软件交付时间变长、风险提升以及运维成本的增加。
14.3 云原生架构相关技术
14.3.1 容器技术
1.容器技术的背景与价值
容器作为标准化软件单元，它将应用及其所有依赖项打包，使应用不再受环境限制，在不
同计算环境间快速、可靠地运行。容器部署模式与其他模式的比较如图14-3所示。
APP APP APP
Operating System
HardWare
Traditional Deployment
APP
Bin/Library
Operating System
Virtual Machine
Operating System
HardWare
Virtualized Deployment
APP APP APP
Bin/Library
Operating System
App App App
Bin/Library Bin/Library Bin/Library
Container Container Container
Container RuntimeVirtual Machine
Operating System
HardWare
Container Deployment
图14-3 传统、虚拟化、容器部署模式比较
虽然2008年Linux提供了Cgroups资源管理机制、Linux Name Space视图隔离方案，让应
用得以运行在独立沙箱环境中，避免相互间冲突与影响；但直到Docker容器引擎的开源，才很
大程度上降低了容器技术的使用复杂性，加速了容器技术普及。Docker容器基于操作系统虚拟
化技术，共享操作系统内核、轻量、没有资源损耗、秒级启动，极大提升了系统的应用部署密
度和弹性。更重要的是，Docker 提出了创新的应用打包规范——Docker镜像，解耦了应用与运
行环境，使应用可以在不同计算环境一致、可靠地运行。借助容器技术呈现了一个优雅的抽象
场景：让开发所需要的灵活性、开放性和运维所关注的标准化、自动化达成相对平衡。容器镜
像迅速成为了应用分发的工业标准。
随后开源的Kubernetes,凭借优秀的开放性、可扩展性以及活跃开发者社区，在容器编排
--- Page 504 ---
第14章 云原生架构设计理论与实践 493
之战中脱颖而出，成为分布式资源调度和自动化运维的事实标准。Kubernetes 屏蔽了IaaS层基
础架构的差异并凭借优良的可移植性，帮助应用一致地运行在包括数据中心、云、边缘计算在
内的不同环境。企业可以通过Kubernetes,结合自身业务特征来设计自身云架构，从而更好地
支持多云/混合云，免去被厂商锁定的顾虑。伴随着容器技术逐步标准化，进一步促进了容器
生态的分工和协同。基于Kubemetes,生态社区开始构建上层的业务抽象，比如服务网格 Istio、
机器学习平台Kubeflow、无服务器应用框架 Knative等。在过去几年，容器技术获得了越发广
泛的应用的同时，三个核心价值最受用户关注：敏捷弹性可移植性容器技术提升企业IT架构敏
捷性的同时，让业务迭代更加迅捷，为创新探索提供了坚实的技术保障。比如疫情期间，教育、
视频、公共健康等行业的在线化需求突现爆发性高速增长，很多企业通过容器技术适时把握了
突如其来的业务快速增长机遇。据统计，使用容器技术可以获得3~10倍交付效率提升，这意味
着企业可以更快速地迭代产品，更低成本进行业务试错。在互联网时代，企业IT系统经常需要
面对促销活动、突发事件等各种预期内外的爆发性流量增长。通过容器技术，企业可以充分发
挥云计算弹性优势，降低运维成本。一般而言，借助容器技术，企业可以通过部署密度提升和
弹性降低50??计算成本。以在线教育行业为例，面对疫情之下指数级增长的流量，教育信息
化应用工具提供商希沃(Seewo)利用阿里云容器服务ACK和弹性容器实例 ECI大大满足了快
速扩容的迫切需求，为数十万名老师提供了良好的在线授课环境，帮助百万学生进行在线学习。
容器已经成为应用分发和交付的标准技术，将应用与底层运行环境进行解耦；Kubemetes 成为资
源调度和编排的标准，屏蔽了底层架构差异性，帮助应用平滑运行在不同基础设施上。CNCF
云原生计算基金会推出了Kubernetes一致性认证，进一步保障了不同K8s实现的兼容性，这也
让企业愿意采用容器技术来构建云时代应用基础设施。
2.容器编排
Kubemetes 已经成为容器编排的事实标准，被广泛用于自动部署，扩展和管理容器化应用。
Kubermetes 提供了分布式应用管理的核心能力。
● 资源调度：根据应用请求的资源量CPU、Memory,或者GPU等设备资源，在集群中选
择合适的节点来运行应用。
●应用部署与管理：支持应用的自动发布与应用的回滚，以及与应用相关的配置的管理；
也可以自动化存储卷的编排，让存储卷与容器应用的生命周期相关联。
● 自动修复：Kubernetes能监测这个集群中所有的宿主机，当宿主机或者OS出现故障，
节点健康检查会自动进行应用迁移；K8s也支持应用的自愈，极大简化了运维管理的复
杂性。
●服务发现与负载均衡：通过Service资源出现各种应用服务，结合DNS和多种负载均衡机
制，支持容器化应用之间的相互通信。
● 弹性伸缩：K8s可以监测业务上所承担的负载，如果这个业务本身的CPU利用率过高，
或者响应时间过长，它可以对这个业务进行自动扩容。
Kubernetes的控制平面包含四个主要的组件：APIServer、Controller、Scheduler以及etcd。
●声明式API:开发者可以关注于应用自身，而非系统执行细节。比如Deployment(无状
--- Page 505 ---
494 系统架构设计师教程(第2版)
态应用)、StatefulSet(有状态应用)、Job(任务类应用)等不同资源类型，提供了对
不同类型工作负载的抽象；对Kubernetes实现而言，基于声明式API的“level-triggered”
实现比“edge-triggered”方式可以提供更加健壮的分布式系统实现。
●可扩展性架构：所有K8s组件都是基于一致的、开放的API实现和交互；三方开发者也可
通过CRD(Custom Resource Definition)/Operator等方法提供领域相关的扩展实现，极大
提升了K8s的能力。
●可移植性：K8s通过一系列抽象如Load Balance Service(负载均衡服务)、CNI(容器网
络接口)、CSI(容器存储接口),帮助业务应用可以屏蔽底层基础设施的实现差异，
实现容器灵活迁移的设计目标。
14.3.2 云原生微服务
1.微服务发展背景
过去开发一个后端应用最为直接的方式就是通过单一后端应用提供并集成所有的服务，即
单体模式。随着业务发展与需求不断增加，单体应用功能愈发复杂，参与开发的工程师规模可
能由最初几个人发展到十几人，应用迭代效率由于集中式研发、测试、发布、沟通模式而显著
下滑。为了解决由单体应用模型衍生的过度集中式项目迭代流程，微服务模式应运而生。
微服务模式将后端单体应用拆分为松耦合的多个子应用，每个子应用负责一组子功能。这
些子应用称为“微服务”,多个“微服务”共同形成了一个物理独立但逻辑完整的分布式微服务
体系。这些微服务相对独立，通过解耦研发、测试与部署流程，提高整体迭代效率。此外，微
服务模式通过分布式架构将应用水平扩展和冗余部署，从根本上解决了单体应用在拓展性和稳
定性上存在的先天架构缺陷。但也要注意到微服务模型也面临着分布式系统的典型挑战：如何
高效调用远程方法、如何实现可靠的系统容量预估、如何建立负载均衡体系、如何面向松耦合
系统进行集成测试、如何面向大规模复杂关联应用的部署与运维。
在云原生时代，云原生微服务体系将充分利用云资源的高可用和安全体系，让应用获得更
有保障的弹性、可用性与安全性。应用构建在云所提供的基础设施与基础服务之上，充分利用
云服务所带来的便捷性、稳定性，降低应用架构的复杂度。云原生的微服务体系也将帮助应用
架构全面升级，让应用天然具有更好的可观测性、可控制性、可容错性等特性。
2.微服务设计约束
相较于单体应用，微服务架构的架构转变，在提升开发、部署等环节灵活性的同时，也提
升了在运维、监控环节的复杂性。设计一个优秀的微服务系统应遵循以下设计约束：
1)微服务个体约束
一个设计良好的微服务应用，所完成的功能在业务域划分上应是相互独立的。与单体应用
强行绑定语言和技术栈相比，这样做的好处是不同业务域有不同的技术选择权，比如推荐系统
采用Python实现效率可能比 Java要高效得多。从组织上来说，微服务对应的团队更小，开发效
率也更高。“一个微服务团队一顿能吃掉两张披萨饼”“一个微服务应用应当能至少两周完成一
--- Page 506 ---
第14章 云原生架构设计理论与实践 495
次迭代”,都是对如何正确划分微服务在业务域边界的隐喻和标准。总结来说，微服务的“微”
并不是为了微而微，而是按照问题域对单体应用做合理拆分。
进一步，微服务也应具备正交分解特性，在职责划分上专注于特定业务并将之做好，即
SOLID原则中单一职责原则(Single Responsibility Principle,SRP)。如果当一个微服务修改或
者发布时，不应该影响到同一系统里另一个微服务的业务交互。
2)微服务与微服务之间的横向关系
在合理划分好微服务间的边界后，主要从微服务的可发现性和可交互性处理服务间的横向
关系。微服务的可发现性是指当服务A发布和扩缩容的时候，依赖服务A的服务B如何在不重
新发布的前提下，如何能够自动感知到服务A的变化?这里需要引入第三方服务注册中心来满
足服务的可发现性；特别是对于大规模微服务集群，服务注册中心的推送和扩展能力尤为关键。
微服务的可交互性是指服务A采用什么样的方式可以调用服务B。由于服务自治的约束，服务
之间的调用需要采用与语言无关的远程调用协议，比如REST协议很好地满足了“与语言无关”
和“标准化”两个重要因素，但在高性能场景下，基于IDL的二进制协议可能是更好的选择。
另外，目前业界大部分微服务实践往往没有达到HATEOAS启发式的REST调用，服务与服务
之间需要通过事先约定接口来完成调用。为了进一步实现服务与服务之间的解耦，微服务体系
中需要有一个独立的元数据中心来存储服务的元数据信息，服务通过查询该中心来理解发起调
用的细节。伴随着服务链路的不断变长，整个微服务系统也就变得越来越脆弱，因此面向失败
设计的原则在微服务体系中就显得尤为重要。对于微服务应用个体，限流、熔断、隔仓、负载
均衡等增强服务韧性的机制成为了标配。为进一步提升系统吞吐能力、充分利用好机器资源，
可以通过协程、Rx模型、异步调用、反压等手段来实现。
3)微服务与数据层之间的纵向约束
在微服务领域，提倡数据存储隔离(Data Storage Segregation,DSS)原则，即数据是微服
务的私有资产，对于该数据的访问都必须通过当前微服务提供的API来访问。如若不然，则
造成数据层产生耦合，违背了高内聚低耦合的原则。同时，出于性能考虑，通常采取读写分离
(CQRS)手段。同样，由于容器调度对底层设施稳定性的不可预知影响，微服务的设计应当尽
量遵循无状态设计原则，这意味着上层应用与底层基础设施的解耦，微服务可以自由在不同容
器间被调度。对于有数据存取(即有状态)的微服务而言，通常使用计算与存储分离方式，将
数据下沉到分布式存储，通过这个方式做到一定程度的无状态化。
4)全局视角下的微服务分布式约束
从微服务系统设计一开始，就需要考虑以下因素：高效运维整个系统，从技术上要准备全
自动化的CI/CD流水线满足对开发效率的诉求，并在这个基础上支持蓝绿、金丝雀等不同发布
策略，以满足对业务发布稳定性的诉求。面对复杂系统，全链路、实时和多维度的可观测能力
成为标配。为了及时、有效地防范各类运维风险，需要从微服务体系多种事件源汇聚并分析相
关数据，然后在中心化的监控系统中进行多维度展现。伴随着微服务拆分的持续，故障发现时
效性和根因精确性始终是开发运维人员的核心诉求。
--- Page 507 ---
496 系统架构设计师教程(第2版)
3.主要微服务技术
Apache Dubbo作为源自阿里巴巴的一款开源高性能RPC框架，特性包括基于透明接口的
RPC、智能负载均衡、自动服务注册和发现、可扩展性高、运行时流量路由与可视化的服务治
理。经过数年发展已是国内使用最广泛的微服务框架并构建了强大的生态体系。为了巩固Dubbo
生态的整体竞争力，2018年阿里巴巴陆续开源了Spring Cloud Alibaba(分布式应用框架)、Nacos
(注册中心&配置中心)、Sentinel(流控防护)、Seata(分布式事务)、Chaosblade(故障注入),
以便让用户享受阿里巴巴十年沉淀的微服务体系，获得简单易用、高性能、高可用等核心能力。
Dubbo在v3中发展服务网格(ServiceMesh),目前Dubbo 协议已经被 Envoy支持，数据层选址、
负载均衡和服务治理方面的工作还在继续，控制层目前在继续丰富Istio/Pilot-discovery中。
Spring Cloud作为开发者的主要微服务选择之一，为开发者提供了分布式系统需要的配置
管理、服务发现、断路器、智能路由、微代理、控制总线、一次性 Token、全局锁、决策竞选、
分布式会话与集群状态管理等能力和开发工具。
Eclipse MicroProfile作为Java微服务开发的基础编程模型，它致力于定义企业Java 微服务
规范，MicroProfile提供指标、API文档、运行状况检查、容错与分布式跟踪等能力，使用它创
建的云原生微服务可以自由地部署在任何地方，包括服务网格架构。
Tars是腾讯将其内部使用的微服务框架TAF(Total ApplicationFramework)多年的实践成
果总结而成的开源项目，在腾讯内部有上百个产品使用，服务内部数千名C+、Java、Golang、
Node.Js与PHP开发者。Tars包含一整套开发框架与管理平台，兼顾多语言、易用性、高性能
与服务治理，理念是让开发更聚焦业务逻辑，让运维更高效。
SOFAStack(Scalable Open Financial Architecture Stack)是由蚂蚁金服开源的一套用于快速
构建金融级分布式架构的中间件，也是在金融场景里的最佳实践。MOSN是SOFAStack 的组件，
它一款采用Go语言开发的服务网格数据平面代理，功能和定位类似Envoy,旨在提供分布式、
模块化、可观测、智能化的代理能力。MOSN 支持Envoy和Istio的API,可以和Istio集成。
DAPR(Distributed Application Runtime,分布式应用运行时)是微软新推出的一种可移植
的、无服务器的、事件驱动的运行时，它使开发人员可以轻松构建弹性，无状态和有状态微服
务，这些服务运行在云和边缘上，并包含多种语言和开发框架。
14.3.3 无服务器技术
1.技术特点
随着以Kubernetes为代表的云原生技术成为云计算的容器界面，Kubernetes 成为云计算的
新一代操作系统。面向特定领域的后端云服务(BaaS)则是这个操作系统上的服务API,存储、
数据库、中间件、大数据、AI等领域的大量产品与技术都开始提供全托管的云形态服务，如今
越来越多用户已习惯使用云服务，而不是自己搭建存储系统、部署数据库软件。
当这些 BaaS云服务日趋完善时，无服务器技术(Serverless)因为屏蔽了服务器的各种运
维复杂度，让开发人员可以将更多精力用于业务逻辑设计与实现，而逐渐成为云原生主流技术
之一。Serverless 计算包含以下特征：
--- Page 508 ---
第14章 云原生架构设计理论与实践 497
(1)全托管的计算服务，客户只需要编写代码构建应用，无需关注同质化的、负担繁重的
基于服务器等基础设施的开发、运维、安全、高可用等工作；
(2)通用性，结合云BaaSAPI的能力，能够支撑云上所有重要类型的应用；
(3)自动弹性伸缩，让用户无需为资源使用提前进行容量规划；
(4)按量计费，让企业使用成本得有效降低，无需为闲置资源付费。
函数计算(Function as a Service,FaaS)是Serverless 中最具代表性的产品形态。通过把应
用逻辑拆分多个函数，每个函数都通过事件驱动的方式触发执行，例如，当对象存储(OSS)
中产生的上传/删除对象等事件，能够自动、可靠地触发FaaS函数处理且每个环节都是弹性和
高可用的，客户能够快速实现大规模数据的实时并行处理。同样，通过消息中间件和函数计算
的集成，客户可以快速实现大规模消息的实时处理。
目前函数计算这种 Serverless形态在普及方面仍存在一定困难，例如：
(1)函数编程以事件驱动方式执行，这在应用架构、开发习惯方面，以及研发交付流程上
都会有比较大的改变；
(2)函数编程的生态仍不够成熟，应用开发者和企业内部的研发流程需要重新适配；
(3)细颗粒度的函数运行也引发了新技术挑战，比如冷启动会导致应用响应延迟，按需建
立数据库连接成本高等。
针对这些情况，在Serverless 计算中又诞生出更多其他形式的服务形态，典型的就是和容器
技术进行融合创新，通过良好的可移植性，容器化的应用能够无差别地运行在开发机、自建机
房以及公有云环境中；基于容器工具链能够加快解决 Serverless的交付。云厂商如阿里云提供了
弹性容器实例(ECI)以及更上层的 Serverless应用引擎(SAE),Google提供了CloudRun服务，
这都帮助用户专注于容器化应用构建，而无需关心基础设施的管理成本。此外Google 也开源了
基于Kubernetes的Serverless应用框架 Knative。
相对函数计算的编程模式，这类Serverless 应用服务支持容器镜像作为载体，无需修改即可部
署在 Serverless环境中，可以享受 Serverless 带来的全托管免运维、自动弹性伸缩、按量计费等优势。
2.技术关注点
1)计算资源弹性调度
为了实现精准、实时的实例伸缩和放置，必须把应用负载的特征作为资源调度依据，使用
“白盒”调度策略，由Serverless平台负责管理应用所需的计算资源。平台要能够识别应用特征，
在负载快速上升时，及时扩容计算资源，保证应用性能稳定；在负载下降时，及时回收计算资
源，加快资源在不同租户函数间的流转，提高数据中心利用率。因此更实时、更主动、更智能
的弹性伸缩能力是函数计算服务获得良好用户体验的关键。通过计算资源的弹性调度，帮助用
户完成指标收集、在线决策、离线分析、决策优化的闭环。在创建新实例时，系统需要判断如
何将应用实例放置在下层计算节点上。放置算法应当满足如下多方面的目标。
(1)容错：当有多个实例时，将其分布在不同的计算节点和可用区上，提高应用的可用性。
(2)资源利用率：在不损失性能的前提下，将计算密集型、I/O密集型等应用调度到相同计
算节点上，尽可能充分利用节点的计算、存储和网络资源。动态迁移不同节点上的碎片化实例，
--- Page 509 ---
498 系统架构设计师教程(第2版)
进行“碎片整理”,提高资源利用率。
(3)性能：例如复用启动过相同应用实例或函数的节点、利用缓存数据加速应用的启动时间。
(4)数据驱动：除了在线调度，系统还将天、周或者更大时间范围的数据用于离线分析。
离线分析的目的是利用全量数据验证在线调度算法的效果，为参数调优提供依据，通过数据驱
动的方式加快资源流转速度，提高集群整体资源利用率。
2)负载均衡和流控
资源调度服务是Serverless系统的关键链路。为了支撑每秒近百万次的资源调度请求，系统
需要对资源调度服务的负载进行分片，横向扩展到多台机器上，避免单点瓶颈。分片管理器通
过监控整个集群的分片和服务器负载情况，执行分片的迁移、分裂、合并操作，从而实现集群
处理能力的横向扩展和负载均衡。在多租户环境下，流量隔离控制是保证服务质量的关键。由
于用户是按实际使用的资源付费，因此计算资源要通过被不同用户的不同应用共享来降低系统
成本。这就需要系统具备出色的隔离能力，避免应用相互干扰。
3)安全性
Serverless计算平台的定位是通用计算服务，要能执行任意用户代码，因此安全是不可逾越
的底线。系统应从权限管理、网络安全、数据安全、运行时安全等各个维度全面保障应用的安
全性。轻量安全容器等新的虚拟化技术实现了更小的资源隔离粒度、更快的启动速度、更小的
系统开销，使数据中心的资源使用变得更加细粒度和动态化，从而更充分地利用碎片化资源。
14.3.4 服务网格
1.技术特点
服务网格(ServiceMesh)是分布式应用在微服务软件架构之上发展起来的新技术，旨在将那
些微服务间的连接、安全、流量控制和可观测等通用功能下沉为平台基础设施，实现应用与平台
基础设施的解耦。这个解耦意味着开发者无需关注微服务相关治理问题而聚焦于业务逻辑本身，
提升应用开发效率并加速业务探索和创新。换句话说，因为大量非功能性从业务进程剥离到另外
进程中，服务网格以无侵入的方式实现了应用轻量化，图14-4展示了服务网格的典型架构。
Istio Mesh
Data plane
Ingress Traffic-L
Service A
Proxy Mesh traffic
Service B
Proxy Egress Traffic+
Discovery Configuration
Certificates
Contral plane
Istiod Pliot Citadel Galley
图14-4 服务网格的典型架构
--- Page 510 ---
第14章 云原生架构设计理论与实践 499
在这张架构图中，服务A调用服务B的所有请求，都被其下的服务代理截获，代理服务
A完成到服务B的服务发现、熔断、限流等策略，而这些策略的总控是在控制平面(Control
Plane)上配置。
从架构上，以开源的Istio服务网格为例，其可以运行在虚拟机或容器中，Istio的主要组件
包括Pilot(服务发现、流量管理)、Mixer(访问控制、可观测性)、Citadel(终端用户认证、流
量加密);整个服务网格关注连接和流量控制、可观测性、安全和可运维性。虽然相比较没有服
务网格的场景多了4个IPC通信的成本，但整体调用的延迟随着软硬件能力的提升而并不会带
来显著的影响，特别是对于百毫秒级别的业务调用而言可以控制在2??内。从另一方面，服
务化的应用并没有做任何改造，就获得了强大的流量控制能力、服务治理能力、可观测能力、4
个9(99.99???上高可用、容灾和安全等能力，加上业务的横向扩展能力，整体收益仍然是
远大于额外IPC通信支出。
服务网格的技术发展上数据平面与控制平面间的协议标准化是必然趋势。大体上，服务网
格的技术发展围绕着事实标准去展开——共建各云厂商共同采纳的开源软件。从接口规范的角
度：Istio采纳了Envoy所实现的xDS协议，将该协议当作是数据平面和控制平面间的标准协议；
Microsoft提出了SMI(Service Mesh Interface),致力于让数据平面和控制平面的标准化做更高
层次的抽象，以期为Istio、Linkerd 等服务网格解决方案在服务观测、流量控制等方面实现最大
程度的开源能力复用。UDPA(Universal Data Plane API)是基于xDS协议而发展起来，以便根
据不同云厂商的特定需求便捷地进行扩展并由xDS去承载。
此外数据平面插件的扩展性和安全性也得到了社区的广泛重视。从数据平面角度，Envoy
得到了包括Google、IBM、Cisco、Microsoft、阿里云等大厂的参与共建以及主流云厂商的采
纳而成为了事实标准。在Envoy的软件设计为插件机制提供了良好扩展性的基础之上，目前
正在探索将Wasm 技术运用于对各种插件进行隔离，避免因为某一插件的软件缺陷而导致整
个数据平面不可用。Wasm技术的优势除了提供沙箱功能外，还能很好地支持多语言，最大
程度地让掌握不同编程语言的开发者可以使用自己所熟悉的技能去扩展Envoy的能力。在安
全方面，服务网格和零信任架构天然有很好的结合，包括PODIdentity、基于mTLS的链路层
加密、在RPC上实施RBAC的ACL、基于Identity 的微隔离环境(动态选取一组节点组成安
全域)。
2.主要技术
2017年发起的服务网格Istio开源项目，清晰定义了数据平面(由开源软件 Envoy承载)和
管理平面(Istio自身的核心能力)。Istio为微服务架构提供了流量管理机制，同时亦为其他增
值功能(包括安全性、监控、路由、连接管理与策略等)创造了基础。Istio利用久经考验的
LyfEnvoy代理进行构建，可在无需对应用程序代码作出任何发动的前提下实现可视性与控制能
力。2019年Istio所发布的1.12版已达到小规模集群上线生产环境水平，但其性能仍受业界诟
病。开源社区正试图通过架构层面演进改善这一问题。由于Istio是建构于Kubernetes 技术之上
的，所以它当然可运行于提供Kubermetes容器服务的云厂商环境中，同时Istio成为了大部分云
厂商默认使用的服务网格方案。
--- Page 511 ---
500 系统架构设计师教程(第2版)
除了Istio外，也有Linkerd、Consul这样相对小众的ServiceMesh解决方案。Linkerd在数
据平面采用了Rust 编程语言实现了linkerd-proxy,控制平面与Istio一样采用Go语言编写。最
新的性能测试数据显示，Linkerd在时延、资源消耗方面比 Istio更具优势。Consul在控制面上
直接使用ConsulServer,在数据面上可以选择性地使用Envoy。与Istio不同的是，Linkerd和
Consul在功能上不如 Istio完整。
Conduit作为Kubernetes的超轻量级 ServiceMesh,其目标是成为最快、最轻、最简单且最
安全的ServiceMesh。它使用Rust构建了快速、安全的数据平面，用Go开发了简单强大的控制
平面，总体设计围绕着性能、安全性和可用性进行。它能透明地管理服务之间的通信，提供可
测性、可靠性、安全性和弹性的支持。虽然与Linkerd相仿，数据平面是在应用代码之外运行的
轻量级代理，控制平面是一个高可用的控制器，然而与Linkerd不同的是，Conduit的设计更加
倾向于Kubernetes 中的低资源部署。
14.4 云原生架构案例分析
随着云计算的普及与云原生的广泛应用，越来越多的从业者、决策者清晰地认识到“云原
生化将成为企业技术创新的关键要素，也是完成企业数字化转型的最短路径”。因此，具有前瞻
思维的互联网企业从应用诞生之初就扎根于云端，谨慎稳重的新零售、政府、金融、医疗等领
域的企业与机构也逐渐将业务应用迁移上云，深度使用云原生技术与云原生架构。面对架构设
计、开发方式到部署运维等不同业务场景，基于云原生架构的应用通常针对云的技术特性进行
技术生命周期设计，最大限度利用云平台的弹性、分布式、自助、按需等产品优势。借助以下
几个典型实践案例，我们来看看企业如何使用云原生架构解决交付周期长、资源利用率低等实
际业务问题。
14.4.1 某旅行公司云原生改造
1.背景和挑战
某旅行公司登录香港联交所主板挂牌上市，成为港股“OTA第一股”。财报显示，2021年
上半年，某艺龙MAU约为2.56亿元，其中在第二季度，MAU达到2.8亿元，同比增长58.3%,
创下了历史新高。业务量的增长让某旅行的技术团队感到欣喜，但另一方面这也意味着团队需
要直面高流量带来的新挑战，云原生改造成了解决问题的关键。
2019年，某旅行公司主要面临两个问题。首先，由于刚和某网完成公司主体合并不久，
两个前身公司各自存在着不同技术体系的构建、发布等系统，这些系统随着公司业务的逐步
整合，也必须在技术层面做进一步的收敛，以达到平台统一的目的。同时，在线旅行业务具
有较明显的业务波动特性，在季度、节假日、每日时段上都有比较突出的波峰波谷特性。这
样的业务特性对技术资源的整体利用率波动影响较大。所以此次云原生改造也面临了不小的
挑战。
--- Page 512 ---
第14章 云原生架构设计理论与实践 501
2.基于云原生架构的解决方案
改造第一阶段，某旅行技术团队为了提升集群资源利用率，降低资源使用成本。利用云原
生思维重构部分技术体系，将多套旧有系统合并、收拢到一套以云原生应用为核心的私有云平
台上，同时将IDC、物理网络、虚拟网络、计算资源、存储资源等通过IaaS、PaaS等，实现虚
拟化封装、切割、再投产的自动化流程。
基础层面，为了支持IaaS层的网络虚拟化，运维人员选择了Vxlan、大二层技术，并用
KVM作为计算资源的切割。在容器网络虚拟化这部分，考虑到要降低损耗，采用了BGP、Host
网络模式等技术，同时开发了绑核、NUMA等相关技术。容器存储方面，远端存储选择了
Ceph,本地层使用块存储设备、NUMA设备等。异构资源侧则采用了GPU改CUDA library的
方式来完成虚拟化的切分和分时复用。技术团队将资源调度变成了利用时序数据预测应用规模
的方式，提升了资源利用率。
但是在改造完成后服务部署时，有大批量的物理机都出现负载上升的情况，原因是低版
本的 Java程序无法准确识别容器里的规格，导致GC时频繁发生资源争抢。由于无法确定其
他语言是否会出现同样的问题，研发团队开发了垂直扩缩容，确保GC可以使用更多的计算
资源。另一方面进行了JVM版本升级，并且还引入了隔离性较强的Kata Container来彻底解
决该问题。
第一阶段改造完成后，平台开始服务同程旅行的大部分在线业务。随着服务器集群规模的
扩大，部分机器开始频繁出现故障。此时，保障服务稳定性成了第二阶段改造的首要任务。
基于公有云、私有云和离线专属云集群等新型动态计算环境，某旅行公司的技术团队帮助
业务构建和运行具有弹性的云原生应用，促进业务团队开始使用声明式API,同时通过不可变
基础设施、服务网格和容器服务，来构建容错性好、易于管理和观察的应用系统，并结合平台
可靠的自动化恢复、弹性计算来完成整个服务稳定性的提升。
技术团队将公有云的镜像预热、分发，专线直连内网机房，解决了内网集群需要镜像快速
分发等问题，依赖的缓存资源和持久化数据实现了常驻云上，离线资源所在的专有云集群也同
步被打通。同时，依托弹性计算能力，团队将集群间资源使用成本降到最低，并将最高服务稳
定性的智能化调度平台的服务动态部署在多个集群上。针对业务专有需求和特殊，平台可以输
出基础设施API和基础能力API,供业务构建自己的云服务。
为了解决应用出现了明显的卡顿，影响到用户体验的问题，团队通过弹性计算改造为业务
快速提供支持，之后又尝试了Scale Zero 等方式，最终将该业务的资源使用量降到了之前常备
资源的20??
2021年上半年，某旅行公司进入到云原生改造的第三个阶段。通过基础组件、服务的云原
生改造、服务依赖梳理和定义等方式，使应用不再需要考虑底层资源、机房、运行时间和供
应商等因素。此外，某旅行公司还利用标准的云原生应用模型，实现了服务的跨地域、跨云
自动化灾备、自动部署，并向云原生场景下的DevOps演进。某旅行公司云原生平台架构图如
图14-5所示。
--- Page 513 ---
502 系统架构设计师教程(第2版)
OS层
在/离线云原生应用 AI/大数据 音/视频 Serverless
云 集群管理 安全隔离 监控&报警 DevOps
多环境&混合云部署 标准&非标资源管控 日志分析 CICD
资源利用率优化 全链路故障漏洞分析 监控报警 资源利用串优化
资源申请收口 应用安全合规审计 全链路追踪 资源申请收口
Jean & Furt(Kubemetes)&兜率(机器学习平台)
鞋
ServiceAPI接口汇聚
Wangler(KVM)
计算
CPU/GPU
机器资源池
Wangler(Docker)
容器网络
Calico/Linux Route
物理机 虚拟机 混合云机器
图14-5 某旅行公司云原生平台架构图
3.应用效益
通过第一阶段改造，订单业务从原先独享机器集群切换到了共享机器集群，仅使用之前独
享机器集群40??机器就完成了对全线服务业务的支撑，同时由于调度算法加入了自研的服务
画像技术作为默认调度属性，资源调度的稳定性不降反升。并且同程旅行已实现纳入到该平台
部分单机资源利用率提升了20并通过云原生化的旧应用改造，下掉了当时集群内一半的服
务器和相应的机房水电资源。
通过第二阶段改造，原本用来应对季节性流量高峰期而采购的机器资源开始减少。通过判
断服务当前冗余度来缩容线上服务的实例数，平台可以用最小的实例数量提供线上服务，而节
省下来的资源可以提供给离线业务混合部署使用。并且在不额外新增机器的情况下额外获得的
算力，成功支持了屡次创纪录的峰值流量。同时 Service Balance系统可以在服务性能受损时自
动尝试修复该节点性能，使得平台能够以较低的成本稳定运行。并借用弹性计算成功撑住爆款
应用带来的日常流量300??峰值流量，也顶住了2021年上半年的屡次刷新公司峰值流量，为
公司同类业务场景提供了坚实的技术支撑。
14.4.2 云原生技术助力某汽车公司数字化转型实践
1.背景和挑战
汽车行业正迅速步入数字化时代。车企服务的对象发生变化，从购车市场转为覆盖后车市
场的全周期，通过互联网渠道直面客户，服务客户急速增多。为适配客户快速变化的需求，互
应用市场&
模型市场&
数据市场
·标准云原生
·标准化数据
·标准化预训
应用
产品
练模型Wangler(Kubernetes)
存储
Local/Route
运行时
Docker/Kata
Linux Windows
--- Page 514 ---
第14章 云原生架构设计理论与实践
联网营销成为常态。业务开展承担新的使命，对业务交付的数量、周期、复杂度都提出新挑战。
目前汽车制造处于“+互联网”和“互联网+”的进程中，面临着互联网业态模式和架构
挑战。对业务价值链进行识别，既要满足对产品配置、价格管理、合同管理等稳态业务的支撑，
又需要实现商机管理、营销策略、营销管理、电子商务等敏捷业务的快速迭代。两种业态将长
期共存。汽车制造正逐渐从传统汽车生产商和销售商，转变为移动服务、自动驾驶和娱乐、车
内体验的供应服务商。新的商业模式，要求充分利用创新的技术，融入业务的每一个角落。软
件定义汽车的时代即将来临。
某汽车公司自2016年开始引入移动互联网、电商等数字化营销系统，逐步布局汽车后服
务市场，为更好更快迎合客户需求变化，掌握市场转换的主动权，对某云行为代表的互联网应
用进行全面的推广，通过触点连接客户并提供便捷用车和增值服务。同时，积极开拓在线支付、
车联网、二手车交易等新型汽车服务业务场景，积累了丰富的实践经验。充分利用容器、微服
务、DevOps 云原生转型方法和手段，驱动技术与汽车场景业务深度融合，建立业务与技术之间
良性循环。
2.基于云原生架构的解决方案
战略性构建容器云平台。通过平台实现对某云行App、二手车、在线支付、优惠券等核心
互联网应用承载。以多租户的形式提供弹性计算、数据持久化、应用发布等面向敏捷业务服务，
并实现高水平资源隔离。标准化交付部署，快速实现业务扩展，满足弹性要求。利用平台健康
检查、智能日志分析和监控告警等手段及时洞察风险，保障云平台和业务应用稳定运行。
数字混合云交付。采用私有云+公有云的混合交付模式，按照服务的敏态/稳态特性和管
控要求划分部署，灵活调度公有云资源来满足临时突发或短期高TPS业务支撑的需求。利用
PaaS平台标准化的环境和架构能力，实现私有云和公有云一致交付体验。
深度融合微服务治理体系，实现架构的革新和能力的沉淀，逐步形成支撑数字化应用的业务中
台(其云平台架构如图146所示)。通过领域设计、系统设计等关键步骤，对原来庞大的某云体系
应用进行微服务拆分，形成能量、社群、用户、车辆、订单等多共享业务服务，同步制定了设计与
开发规范、实施路径和配套设施，形成一整套基于微服务的分布式应用架构规划、设计方法论。
能量订单用户 车辆 …
服务发现消息中心 )(调度中心)配置中心
RabbitMQ Kafka Redis Nginx …
网关
需求；-项目}-开发}-测试}-上线}-叫运维
资源 集群 应用 镜像 安全 日志 监控告警
业务组件
微服务治理
组件
中间件服务
DevOps
容器云平台
测试环境 压测环境 生产环境 共有云环境 ]混合云环境
图14-6 某容器云平台架构示意图
503
--- Page 515 ---
504 系统架构设计师教程(第2版)
DevOps理念贯穿始终。通过DevOps平台规避软件黑盒，从软件生命周期的源头开始把
控，实现对核心代码资产的自主、透明管理，避免对开发商的过度依赖。利用DevOps平台的
可视化界面，实现全流程自动化、透明化、标准化，实现业务功能迭代变更的核心掌控。
3.应用效益
某汽车公司采用云原生技术在多云环境部署混合云平台，推进某云行体系应用的架构革新。
满足多样化的业务上云需求，满足业务高可用、高性能、高扩展性、高伸缩性和高安全性要求，
为互联网场景业务开展提供有效支撑。提升不同场景下的互联网业务资源使用效率，同时建立
以容器为核心的应用交付和运维管理标准，并制定微服务架构应用管理规范。
加强技术管控，提升交付速度。建立适配某汽车公司的DevOps实践规范，配合敏捷化开
发模式，通过DevOps平台实现快速、持续、可靠、规模化地交付业务应用，应用交付周期从
两个月缩短到一个月。
通过敏捷基础架构能力，加快业务创新步伐。持续推进车联网、共享出行等新型场景布局，
为某汽车公司数字化转型发展提供有力支撑。
14.4.3 某快递公司核心业务系统云原生改造
1.背景和挑战
作为发展最为迅猛的物流企业之一，某快递公司一直积极探索技术创新赋能商业增长之路，
以期达到降本提效的目的。目前，某快递公司日订单处理量已达千万量级，亿级别物流轨迹处
理量，每天产生数据已达到TB级别，使用1300+个计算结点来实时处理业务。
过往某快递公司的核心业务应用运行在IDC机房，原有IDC系统帮助某快递公司安稳度过
早期业务快速发展期。但伴随着业务体量指数级增长，业务形式愈发多元化。原有系统暴露出
不少问题，传统IOE架构、各系统架构的不规范、稳定性、研发效率都限制了业务高速发展的
可能。软件交付周期过长，大促保障对资源的特殊要求难实现、系统稳定性难以保障等业务问
题逐渐暴露。
在与阿里云进行多次需求沟通与技术验证后，某快递公司最终确定阿里云为唯一合作伙
伴，采用云原生技术和架构实现核心业务搬迁上阿里云。2019年开始将业务逐步从IDC迁移
至阿里云。目前，核心业务系统已经在阿里云上完成流量承接，为申通提供稳定而高效的计
算能力。
2.基于云原生架构的解决方案
某快递公司核心业务系统原架构基于Vmware+Oracle数据库进行搭建。随着搬迁上阿里
云，架构全面转型为基于Kubernetes的云原生架构体系。其中，引入云原生数据库并完成应用
基于容器的微服务改造是整个应用服务架构重构的关键点。
1)引入云原生数据库
通过引入OLTP跟OLAP型数据库，将在线数据与离线分析逻辑拆分到两种数据库中，改
变此前完全依赖Oracle 数据库的现状。满足在处理历史数据查询场景下 Oracle 数据库所无法支
--- Page 516 ---
第14章 云原生架构设计理论与实践
持的实际业务需求。
2)应用容器化
伴随着容器化技术的引进，通过应用容器化有效解决了环境不一致的问题，确保应用在开
发、测试、生产环境的一致性。与虚拟机相比，容器化提供了效率与速度的双重提升，让应用
更适合微服务场景，有效提升产研效率。
3)微服务改造
由于过往很多业务是基于Oracle的存储过程及触发器完成的，系统间的服务依赖也需要
Oracle 数据库OGG(Oracle Golden Gate)同步完成。这样带来的问题就是系统维护难度高且稳
定性差。通过引入Kubernetes的服务发现，组建微服务解决方案，将业务按业务域进行拆分，
让整个系统更易于维护。
综合考虑申通实际业务需求与技术特征，最终选择了“阿里云ACK+神龙+云数据库”的
云原生解决方案，从而实现核心应用迁移上阿里云。图14-7展示了最终的上云架构。
505
流量入口
接入层
应用中台基础设施
阿里云DNS Linux-Bind
F5-外部VIP F5-内部VIP
外部：sto-express.cn 内部：stosystem.com
Nginx
外网
Nginx
内网
Java
VM
.NET
VM
OracleRedis中间件
VMWare虚拟化平台
物理机
阿里DNS PrivateZone
SLB-外部VIP SLB-内部VIP
外部：sto.cn 内部：stosystem.com
生产-2套Nginx接入
内/外
生产-3套Ingress
内/外/办公网
.NET
ECS
Java
Pod
云上中间件 云Redis OLAP/OLTP
ACK
神龙服务器
上云混合态架构
图14-7 某快递公司核心业务上云架构示意图
(1)架构阐述。
基础设施，全部计算资源取自阿里云的神龙裸金属服务器。相较于一般云服务器(ECS),
Kubernetes 搭配神龙服务器能够获得更优性能及更合理的资源利用率。且云上资源按需取量，
对于拥有促活动等短期大流量业务场景的申通而言极为重要。相较于线下自建机房、常备机器，
云上资源随取随用。在促活动结束后，云上资源使用完毕后即可释放，管理与采购成本更低，
相应效率。
--- Page 517 ---
506 系统架构设计师教程(第2版)
流量接入，阿里云提供两套流量接入，一套是面向公网请求，另外一套是服务内部调用。
域名解析采用云 DNS及PrivateZone。借助Kubernetes的Ingress能力实现统一的域名转发，以
节省公网 SLB的数量，提高运维管理效率。
(2)平台层。
基于 Kubernetes 打造的云原生PaaS平台优势明显突出。
● 打通DevOps闭环，统一测试，集成，预发、生产环境；
● 天生资源隔离，机器资源利用率高；
● 流量接入可实现精细化管理；
● 集成了日志、链路诊断、Metrics平台；
● 统一APIServer接口和扩展，支持多云及混合云部署。
(3)应用服务层。
每个应用都在Kubernetes上面创建单独的一个Namespace,应用和应用之间实现资源隔离。
通过定义各个应用的配置Yaml模板，当应用在部署时直接编辑其中的镜像版本即可快速完成版
本升级，当需要回滚时直接在本地启动历史版本的镜像快速回滚。
(4)运维管理。
线上Kubernetes 集群采用阿里云托管版容器服务，免去了运维Master结点的工作，只需要
制定Worker结点上线及下线流程即可。同时业务系统均通过阿里云的PaaS平台完成业务日志
搜索，按照业务需求投交扩容任务，系统自动完成扩容操作，降低了直接操作Kubernetes 集群
带来的业务风险。
3.应用效益
成本方面：使用公有云作为计算平台，可以让企业不必因为业务突发增长需求，而一次性
投入大量资金成本用于采购服务器及扩充机柜。在公共云上可以做到随用随付，对于一些创新
业务想做技术调研十分便捷。用完即释放，按量付费。另外云产品都免运维自行托管在云端，
有效节省人工运维成本，让企业更专注于核心业务。
稳定性方面：首先，云上产品提供至少5个9(99.999???上的SLA服务确保系统稳定，
而自建系统稳定性相去甚远。其次，部分开源软件可能存在功能Bug,造成故障隐患。最后，
在数据安全方面云上数据可以轻松实现异地备份，阿里云数据存储体系下的归档存储产品具备
高可靠、低成本、安全性、存储无限等特点，让企业数据更安全。
效率方面：借助与云产品深度集成，研发人员可以完成一站式研发、运维工作。从业务
需求立项到拉取分支开发，再到测试环境功能回归验证，最终部署到预发验证及上线，整个
持续集成流程耗时可缩短至分钟级。排查问题方面，研发人员直接选择所负责的应用，并通
过集成的 SLS日志控制台快速检索程序的异常日志进行问题定位，免去了登录机器查日志的
麻烦。
赋能业务：阿里云提供超过300余种的云上组件，组件涵盖计算、AI、大数据、IoT等诸
多领域。研发人员开箱即用，有效节省业务创新带来的技术成本。
--- Page 518 ---
第14章 云原生架构设计理论与实践 507
14.4.4 某电商业务云原生改造
1.背景和挑战
某是一家致力于线上的化妆品销售品牌。伴随着公司业务高速发展，技术运维面临着非常
严峻的挑战。伴随着“双11”电商大促、“双12”购物节、小程序、网红直播带货呈现爆发式
增长趋势，如何确保微商城系统稳定顺畅地运行成为某面对的首要难题。其中，比较突出几个
挑战包含以下几点：
● 系统开发迭代快，线上问题较多，定位问题耗时较长；
●频繁大促，系统稳定性保障压力很大，第三方接口和一些慢SQL存在导致严重线上故障
的风险；
● 压测与系统容量评估工作相对频繁，缺乏常态化机制支撑；
● 系统大促所需资源与日常资源相差较大，需要频繁扩缩容。
2.云原生解决方案
某与阿里云一起针对所面临问题以及未来业务规划进行了深度沟通与研讨。通过阿里云
原生应用稳定性解决方案以解决业务问题。引入阿里云容器服务ACK、Spring Cloud Alibaba、
PTS、AHAS、链路追踪等配套产品，对应用进行容器化改造部署，优化配套的测试、容量评估、
扩缩容等研发环节，提升产研效率。图14-8展示了某最终的核心应用架构方案。
Redis云数据库
PTS
容器 容器
kubernetes 容器 弹性伸缩 容器
容器 容器
性能测试服务
云盘 NAS OSS
图14-8 某核心应用架构示意图
方案的关键点是：
● 通过容器化部署，利用阿里云容器服务的快速弹性应对大促时的资源快速扩容。
● 提前接入链路追踪产品，用于对分布式环境下复杂的服务调用进行跟踪，对异常服务进
行定位，帮助客户在测试和生产中快速定位问题并修复，降低对业务的影响。
●使用阿里云性能测试服务(PTS)进行压测，利用秒级流量拉起、真实地理位置流量等功
能，以最真实的互联网流量进行压测，确保业务上线后的稳定运营。
ARMSAPM类全链路监控
限流、熔断、降级、AHAS 系统保护
监控、报警云监控
MQ消息队列
DB弹性扩容
日志类数据采、消费、
投递集及查询分析功能云日志
--- Page 519 ---
508 系统架构设计师教程(第2版)
● 采集压测数据，解析系统强弱依赖关系、关键瓶颈点，对关键业务接口、关键第三方调
用、数据库慢调用、系统整体负载等进行限流保护。
● 配合阿里云服务团队，在大促前进行ECS/RDS/安全等产品扩容、链路梳理、缓存/连接
池预热、监控大屏制作、后端资源保障演练等，帮助大促平稳进行。
3.应用效益
● 高可用：利用应用高可用服务产品(AHAS)的限流降级和系统防护功能，对系统关
键资源进行防护，并对整体系统水位进行兜底，确保大促平稳进行，确保顺畅的用户
体验。
● 容量评估：利用性能测试服务(PTS)和业务实时监控(ARMS)对系统单机能力及整
体容量进行评估，对单机及整体所能承载的业务极限量进行提前研判，以确保未来对业
务大促需求可以做出合理的资源规划和成本预测。
● 大促保障机制：通过与阿里云服务团队的多次配合演练，建立大促保障标准流程及应急
机制，达到大促保障常态化。
4.客户声音
“使用ACK容器服务可以帮助我们快速拉起测试环境，利用PTS 即时高并发流量压测确认
系统水位，结合ARMS监控，诊断压测过程中的性能瓶颈，最后通过AHAS对突发流量和意外
场景进行实时限流降级，加上阿里云团队保驾护航，保证了我们每一次大促活动的系统稳定性
和可用性，同时利用ACK容器快速弹性扩缩容，节约服务器成本50以上。”某技术中台负责
人如上说。
14.4.5 某体育用品公司基于云原生架构的业务中台构建
1.背景和挑战
某体育用品公司作为中国领先的体育用品企业之一，在2016年，某体育用品公司启动集团
第三次战略升级，打造以消费者体验为核心的“3+”(“互联网+”、“体育+”和“产品+”)的
战略目标，积极拥抱云计算、大数据等新技术，实现业务引领和技术创新，支撑企业战略变革
的稳步推进。在集团战略的促使下，阿里云中间件团队受邀对某体育用品公司IT信息化进行了
深度调研，挖掘阻碍其战略落地的些许挑战：
(1)商业套件导致无法满足某体育用品公司业务多元化发展要求，例如多品牌拆分重组所
涉及的相关业务流程以及组织调整。对某体育用品公司而言，传统应用系统都是紧耦合，业务
的拆分重组意味着必须重新实施部署相关系统。
(2)IT历史包袱严重，内部烟囱系统林立。通过调研，阿里云发现某体育用品公司烟囱
系统多达63套，仅IT供应商就有三十余家。面对线上线下业务整合涉及的销售、物流、生
产、采购、订货会、设计等不同环节及场景，想要实现全渠道整合，需要将几十套系统全部
打通。
--- Page 520 ---
第14章 云原生架构设计理论与实践
(3)高库存、高缺货问题一直是服装行业的死结，某体育用品公司同样被这些问题困扰着。
系统割裂导致数据无法实时在线，并受限于传统单体 SQL Server数据库并发限制，6000多家门
店数据只能采用T+1方式回传给总部，直接影响库存高效协同周转。
(4)IT建设成本浪费比较严重，传统商业套件带来了“烟囱式”系统的弊端，导致很多功
能重复建设、重复数据模型以及不必要的重复维护工作。
2.云原生解决方案
阿里云根据某体育用品公司业务转型战略需求，为之量身打造了基于云原生架构的全渠
道业务中台解决方案，将不同渠道通用功能在云端合并、标准化、共享，衍生出全局共享的
商品中心、渠道中心、库存中心、订单中心、营销中心、用户中心、结算中心。无论哪个业
务线、哪个渠道、哪个新产品诞生或调整，IT组织都能根据业务需求，基于共享服务中心现
有模块快速响应，打破低效的“烟囱式”应用建设方式。全渠道业务中台遵循互联网架构原
则，规划线上线下松耦合云平台架构，不仅彻底摆脱传统IT拖业务后腿的顽疾并实现灵活支
撑业务快速创新，将全渠道数据融通整合在共享服务中心平台上，为数据化决策、精准营销、
统一用户体验奠定了良好的产品与数据基础，让某体育用品公司真正走上了“互联网+”的快
车道。
2017年1月某体育用品公司与阿里云启动全渠道中台建设，耗时6个月完成包括需求
调研、中台设计、研发实施、测试验证等在内的交付部署，历经4个月实现全国42家分
公司、6000+门店全部上线成功。图14-9是某体育用品公司全渠道业务中台总体规划示
意图。
学R台 n店零售POS
金通运云
分销订货828 天层淘宝 官方商址 特跑款APP 特购局城
全果道运营支撑平台
果通分罚管理 nM零督运营 商品订单管理 会员营钥管理 WMS 智能配补同 门店运露
采购管理 销停管理 蛋折管理 促销管理 库存可视 商品管理 会员资科 会员等级 库存管理 门店优先股 参效配里 门店管理
序存管理 聚迎管理 库存管理 业绩管理 可单管理 订单适配 积分体系 开降级管理 库位置理 补货计耳 补货人工
调及 导购管理
货权转移 销售格其 调推管理 发氏管理 订单接入 010站算 会员日管理 会员标签 发园管理 的售围 口店售品率 阳效考板管理 管理
业务共字服务中心
渠道中心 商品中心 序存中心 交品中心
CRM 会负向城 WAS
馅算中心
的告站算服务
020箱算服务
贴耳对账服务
会出中心
往册服务
积分服务
桂运服务
冒的中心
代企券服务
红包服务
团购服务
发付中心
智显导期
数据中心
业中苔
基础服务 商品限务 出入序围务 T单服务 做信服务 批表服务
组职服务 类日服务 血应服务 即欧服务 文付宝服务 统计服务
门阳胆务 餐钢服务 岸存可视 寻罐服务 眼眼服务 标签服务
石白 MESPLMSAP HROA
图14-9 某体育用品公司全渠道业务中台总体规划示意图
图14-10是基于云原生中间件的技术架构示意图。
人定制
509
--- Page 521 ---
510 系统架构设计师教程(第2版)
应用
中心
基础设施
CRMPOS OMS DRP 库存平衡 全渠道运营平台
主数据结算中心库存中心订单中心
商品中心 会员中心 营销中心 ……
ARMSMQEDASK8S PTS CSB 阿里云原生中间件产品
阿里云IaaS服务
后台系统
图14-10 基于云原生中间件的技术架构示意图
架构的关键点：
(1)应用侧：新技术架构全面承载面向不同业务部门的相关应用，包括门店POS、电商
oMS、分销商管理供销存DRP、会员客户管理CRM。此外，在全渠道管理方面也会有一些
智能分析应用，比如库存平衡，同时可以通过全渠道运营平台来简化全渠道的一些配置管理。
所有涉及企业通用业务能力比如商品、订单等，可以直接调用共享中心的能力，让应用“更
轻薄"。
(2)共享中心：全渠道管理涉及参与商品品类、订单寻源、共享库存、结算规则等业务场
景，也涉及与全渠道相关的会员信息与营销活动等。这些通用业务能力全部沉淀到共享中心，
向不同业务部门输出实时、在线、统一、复用的能力。直接将某体育用品公司所有订单、商品、
会员等信息融合、沉淀到一起，从根本上消除数据孤岛。
(3)技术层：为了满足弹性、高可用、高性能等需求，通过Kubernetes、EDAS、MQ、
ARMS、PTS等云原生中间件产品，目前某体育用品公司核心交易链路并发可支撑10w/tps且支
持无线扩容提升并发能力。采用阿里历经多年“双11”考验的技术平台，稳定性和效率都得到
了高规格保障，让开发人员能够更加专注在业务逻辑实现，再无后顾之忧。
(4)基础设施：底层的计算、存储、网络等IaaS层资源。
(5)后台系统：客户内部的后台系统，比如 SAP、生产系统、HR/OA等。
3.应用效益
全渠道业务中台为某体育用品公司核心战略升级带来了明显的变化，逐步实现了IT驱动业
务创新。
经过中台改造后，POS系统从离线升级为在线化。包括收银、库存、会员、营销在内的
POS系统核心业务全部由业务中台统一提供服务，从弱管控转变为集团强管控，集团与消费者
之同真正建立起连接，为消费者精细化管理奠定了坚实的基础。
中台的出现，实现了前端渠道的全局库存共享，库存业务由库存中心实时处理。借助全局
库存可视化，交易订单状态信息在全渠道实时流转，总部可直接根据实时经营数据对线下店铺
送行销售指导，实现快速跨店商品折拨。中台上线后，售罄率提升8缺货率降低12周转
全渠道运营平台库存平衡CRM
--- Page 522 ---
第14章 云原生架构设计理论与实践
率提升20做到赋能一线业务。
IT信息化驱动业务创新，通过共享服务中心将不同渠道类似功能在云端合并共享，打破低
效的“烟囱式”应用建设方式，吸收互联网DDD领域驱动设计原则，设计线上线下松耦合云平
台架构，不仅彻底摆脱了传统IT拖业务后腿的顽疾并灵活支撑业务快速创新。全渠道数据融通
整合在共享服务中心平台上，沉淀和打造出特步的核心数据资产，培养出企业中最稀缺的“精
通业务，懂技术”创新人才，使之在企业业务创新、市场竞争中发挥核心作用。截至2019年
初，业务部门对IT部门认可度持续上升，目前全渠道业务支撑系统几乎全部自主搭建，80?
台应用已经全部运行在中台之上，真正实现技术驱动企业业务创新。
511
--- Page 523 ---
第15章 面向服务架构设计理论与实践
Massimo Pezzini,Gartner Group说过，“当有一天，所有的应用都写成 Web服务，集成也许
可以变得更容易”。
服务是一个由服务提供者提供的，用于满足使用者请求的业务单元。服务的提供者和使用
者都是软件代理为了各自的利益而产生的角色。
在面向服务的体系结构(Service-Oriented Architecture,SOA)中，服务的概念有了延伸，泛
指系统对外提供的功能集。例如，在一个大型企业内部，可能存在进销存、人事档案和财务等多
个系统，在实施 SOA后，每个系统用于提供相应的服务，财务系统作为资金运作的重要环节，也
向整个企业信息化系统提供财务处理的服务，那么财务系统的开放接口可以看成是一个服务。
15.1 SOA的相关概念
15.1.1 SOA的定义
面向服务的体系结构(Service-Oriented Architecture,SOA),从应用和原理的角度看，目前
有两种业界公认的标准定义。
从应用的角度定义，可以认为 SOA是一种应用框架，它着眼于日常的业务应用，并将它们
划分为单独的业务功能和流程，即所谓的服务。SOA使用户可以构建、部署和整合这些服务，
且无需依赖应用程序及其运行平台，从而提高业务流程的灵活性。这种业务灵活性可使企业加
快发展速度，降低总体拥有成本，改善对及时、准确信息的访问。SOA有助于实现更多的资产
重用、更轻松的管理和更快的开发与部署。
从软件的基本原理定义，可以认为 SOA是一个组件模型，它将应用程序的不同功能单元
(称为服务)通过这些服务之间定义良好的接口和契约联系起来。接口是采用中立的方式进行定
义的，它应该独立于实现服务的硬件平台、操作系统和编程语言。这使得构建在各种这样的系
统中的服务可以一种统一和通用的方式进行交互。
作为软件架构师，后一种从软件原理方面的定义，对日常工作更具指导性。
15.1.2 业务流程与BPEL
业务流程是指为了实现某种业务目的行为所进行的流程或一系列动作。在计算机领域，业
务流程代表的是某一个问题在计算机系统内部得到解决的全部流程。
由于业务流程来源于现实世界，传统上是通过复杂的语言进行描述。在计算机业务系统
建模中，需要用到一种特定的、简洁的语言来专门描述计算机系统的业务流程，这便促使了
BPEL的诞生。